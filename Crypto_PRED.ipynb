{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ce81ca-ebea-4183-9373-808d95e393d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Import Libraries\n",
    "# =============================================================================\n",
    "import requests                     # For making API calls to CoinMarketCap\n",
    "import pandas as pd                 # For data manipulation and analysis\n",
    "import numpy as np                  # For numerical operations\n",
    "import matplotlib.pyplot as plt     # For plotting graphs\n",
    "import datetime                     # For handling date and time information\n",
    "import nltk                         # For Natural Language Processing\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # VADER for sentiment analysis\n",
    "from sklearn.linear_model import LinearRegression           # For predictive modeling\n",
    "from sklearn.metrics import mean_absolute_error, r2_score      # For model evaluation\n",
    "\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496a270c-f717-488d-97b3-81a8ed70254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample\n",
      "     id         name symbol      slug  num_market_pairs  \\\n",
      "0     1      Bitcoin    BTC   bitcoin             11976   \n",
      "1  1027     Ethereum    ETH  ethereum             10038   \n",
      "2   825  Tether USDt   USDT    tether            119215   \n",
      "3    52          XRP    XRP       xrp              1550   \n",
      "4  1839          BNB    BNB       bnb              2386   \n",
      "\n",
      "                 date_added  \\\n",
      "0  2010-07-13T00:00:00.000Z   \n",
      "1  2015-08-07T00:00:00.000Z   \n",
      "2  2015-02-25T00:00:00.000Z   \n",
      "3  2013-08-04T00:00:00.000Z   \n",
      "4  2017-07-25T00:00:00.000Z   \n",
      "\n",
      "                                                tags    max_supply  \\\n",
      "0  [mineable, pow, sha-256, store-of-value, state...  2.100000e+07   \n",
      "1  [pos, smart-contracts, ethereum-ecosystem, coi...           NaN   \n",
      "2  [stablecoin, asset-backed-stablecoin, ethereum...           NaN   \n",
      "3  [medium-of-exchange, enterprise-solutions, xrp...  1.000000e+11   \n",
      "4  [marketplace, centralized-exchange, payments, ...           NaN   \n",
      "\n",
      "   circulating_supply  total_supply  ...  percent_change_1h  \\\n",
      "0        1.983776e+07  1.983776e+07  ...          -1.047209   \n",
      "1        1.206192e+08  1.206192e+08  ...          -1.193673   \n",
      "2        1.434650e+11  1.445343e+11  ...          -0.011903   \n",
      "3        5.810892e+10  9.998634e+10  ...          -1.212023   \n",
      "4        1.424747e+08  1.424747e+08  ...          -1.109940   \n",
      "\n",
      "  percent_change_24h  percent_change_7d percent_change_30d percent_change_60d  \\\n",
      "0          -1.434840           0.237473         -15.338817         -16.728409   \n",
      "1          -2.848135          -8.136759         -31.677826         -45.109897   \n",
      "2          -0.010185           0.009256          -0.021960          -0.015131   \n",
      "3          -4.247544           5.892523         -17.735852         -21.859959   \n",
      "4          -2.241907           6.782793         -10.480166         -15.399996   \n",
      "\n",
      "  percent_change_90d    market_cap  market_cap_dominance  \\\n",
      "0         -21.980428  1.646457e+12               60.8896   \n",
      "1         -53.711965  2.262286e+11                8.3758   \n",
      "2          -0.000982  1.434594e+11                5.3114   \n",
      "3          -6.768328  1.335601e+11                4.9449   \n",
      "4         -17.758076  8.518145e+10                3.1537   \n",
      "\n",
      "   fully_diluted_market_cap   tvl  \n",
      "0              1.742918e+12  None  \n",
      "1              2.262286e+11  None  \n",
      "2              1.445286e+11  None  \n",
      "3              2.298444e+11  None  \n",
      "4              8.518145e+10  None  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Function to Fetch Crypto Data From CoinMarketCap API\n",
    "# =============================================================================\n",
    "def get_crypto_data(api_key, start=1, limit=5, convert='USD'):\n",
    "    \"\"\"\n",
    "    Gets crypto listings form CMC API(CoinMarketCAP)\n",
    "\n",
    "    Parameters:\n",
    "    api_key: CMC API KEY\n",
    "    start: This the starting rank(the crypto we're choosing/starting from. Set to 1 for Bitcoin)\n",
    "    limit: This is the amount of cryptocurrencies to retrieve\n",
    "    convert: the fiat currency to convert the prices\n",
    "\n",
    "    Returns:\n",
    "       Dataframe: A panda df containing crypto data \n",
    "    \"\"\"\n",
    "    url = \"https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest\"\n",
    "\n",
    "\n",
    "    parameters = {\n",
    "        'start': str(start),\n",
    "        'limit': str(limit),\n",
    "        'convert': convert \n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Accepts': 'application/json',\n",
    "        'X-CMC_PRO_API_KEY': api_key\n",
    "    }\n",
    "\n",
    "    #API Request\n",
    "    response = requests.get(url, headers=headers, params=parameters)\n",
    "    data = response.json()\n",
    "\n",
    "    #Turns Crypto list to dataframe\n",
    "    crypto_list = data['data']\n",
    "    df = pd.DataFrame(crypto_list)\n",
    "\n",
    "    # Convert the nested 'quote' column(dictionary) into columns\n",
    "    df_quote = df['quote'].apply(lambda x: x[convert])\n",
    "    df_quote = pd.json_normalize(df_quote)\n",
    "\n",
    "    if 'last_updated' in df_quote.columns:\n",
    "        df_quote = df_quote.drop(columns=['last_updated'])\n",
    "\n",
    "    # Combine the data\n",
    "    df = pd.concat([df.drop(columns=['quote']), df_quote], axis=1)\n",
    "\n",
    "    #Convert the \"last_updated\" column (from quote but concated to df)\n",
    "    df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "    return df\n",
    "     \n",
    "api_key = 'c936ec13-bf84-4335-b1df-aa936ab8d2b5'\n",
    "cryp_df = get_crypto_data(api_key, start=1, limit=5)\n",
    "print(\"Data Sample\")\n",
    "print(cryp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d5f00-c6ef-4d35-8253-c1f56e6076f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No posts were returned. Here is the raw JSON response for debugging:\n",
      "{'detail': 'Not authenticated'}\n",
      "\n",
      "Reddit Posts Sample:\n",
      "Empty DataFrame\n",
      "Columns: [created_utc, text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Function: get_reddit_posts\n",
    "# =============================================================================\n",
    "def get_reddit_posts(subreddit, size=100, before=None, after=None):\n",
    "    \"\"\"\n",
    "    Fetches Reddit posts from a given subreddit using the Pushshift API. With this we don't need\n",
    "    to get a separate reddit api and we can specify the date range we want the data from\n",
    "    \n",
    "    Parameters:\n",
    "        subreddit (str): Name of the subreddit (e.g., 'cryptocurrency').\n",
    "        size (int): Number of posts to retrieve (default 100).\n",
    "        before (int): (Optional) Timestamp to get posts before this time.\n",
    "        after (int): (Optional) Timestamp to get posts after this time.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing post timestamps and text.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    # Build the parameters for the API call\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': size,\n",
    "        'sort': 'desc',\n",
    "        'sort_type': 'created_utc'\n",
    "    }\n",
    "    if before:\n",
    "        params['before'] = before\n",
    "    if after:\n",
    "        params['after'] = after\n",
    "    \n",
    "    # Make the API request to Pushshift\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()  # Convert the response into a Python dictionary\n",
    "    \n",
    "    posts = data.get('data', [])\n",
    "    \n",
    "    #handles missing created_utc error\n",
    "    if not posts:\n",
    "        print(\"No posts were returned. Here is the raw JSON response for debugging:\")\n",
    "        print(data)\n",
    "        # Return an empty DataFrame to avoid further errors\n",
    "        return pd.DataFrame(columns=['created_utc', 'text'])\n",
    "    \n",
    "    \n",
    "    posts = data.get('data', [])\n",
    "    post_data = []\n",
    "    # For each post, extract the timestamp and combine title with selftext for analysis\n",
    "    for post in posts:\n",
    "        created_utc = post.get('created_utc')\n",
    "        # Combine title and selftext (if available)\n",
    "        title = post.get('title', '')\n",
    "        selftext = post.get('selftext', '')\n",
    "        text = title + \" \" + selftext\n",
    "        post_data.append({'created_utc': created_utc, 'text': text})\n",
    "    \n",
    "    # Convert the list of posts into a DataFrame\n",
    "    df_posts = pd.DataFrame(post_data)\n",
    "    \n",
    "    #To handle the empty dataset error\n",
    "    if df_posts.empty:\n",
    "        print(\"No posts with 'created_utc' were found. Check the API response or parameters.\")\n",
    "        return df_posts\n",
    "    \n",
    "    # Convert UNIX timestamps (seconds since epoch) to datetime objects\n",
    "    df_posts['created_utc'] = pd.to_datetime(df_posts['created_utc'], unit='s')\n",
    "    return df_posts\n",
    "\n",
    "# Example usage: Fetch posts from the 'cryptocurrency' subreddit\n",
    "reddit_posts_df = get_reddit_posts(subreddit='cryptocurrency', size=200)\n",
    "print(\"\\nReddit Posts Sample:\")\n",
    "print(reddit_posts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866a17dc-90d2-4af1-ba93-6329319cd203",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reddit_cryptocurrency_posts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load Reddit Data from Kaggle\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read the CSV file containing Reddit posts. The file should include columns like:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 'created_utc', 'title', and 'selftext'.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m reddit_kaggle_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreddit_cryptocurrency_posts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# If there is no 'text' column, combine 'title' and 'selftext' to form a complete post text.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m reddit_kaggle_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\DBABA\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\DBABA\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\DBABA\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\DBABA\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\DBABA\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_cryptocurrency_posts.csv'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Reddit Data from Kaggle\n",
    "# =============================================================================\n",
    "# Read the CSV file containing Reddit posts. The file should include columns like:\n",
    "# 'created_utc', 'title', and 'selftext'.\n",
    "reddit_kaggle_df = pd.read_csv('reddit_cryptocurrency_posts.csv')\n",
    "\n",
    "# If there is no 'text' column, combine 'title' and 'selftext' to form a complete post text.\n",
    "if 'text' not in reddit_kaggle_df.columns:\n",
    "    reddit_kaggle_df['text'] = reddit_kaggle_df['title'].fillna('') + \" \" + reddit_kaggle_df['selftext'].fillna('')\n",
    "\n",
    "# Convert the 'created_utc' column to datetime.\n",
    "# If the values are UNIX epoch seconds, specify unit='s'; otherwise, let pandas infer the format.\n",
    "if reddit_kaggle_df['created_utc'].dtype in [int, float]:\n",
    "    reddit_kaggle_df['created_utc'] = pd.to_datetime(reddit_kaggle_df['created_utc'], unit='s')\n",
    "else:\n",
    "    reddit_kaggle_df['created_utc'] = pd.to_datetime(reddit_kaggle_df['created_utc'])\n",
    "\n",
    "print(\"Loaded Reddit Data from Kaggle:\")\n",
    "print(reddit_kaggle_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8232e2-e84f-41bf-9082-0e59275ce83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
